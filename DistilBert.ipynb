{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, TrainingArguments, Trainer, EvalPrediction\n",
    "from datasets import load_from_disk, concatenate_datasets, DatasetDict\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk('data/neural_data_post_process/')\n",
    "\n",
    "train_data = concatenate_datasets([dataset['train'],dataset['valid'].select(range(842)), dataset['test'].select(range(2000))])\n",
    "valid_data = dataset['valid'].select(range(842,2842,1))\n",
    "test_data = dataset['test'].select(range(2000,2842,1))\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': train_data,\n",
    "    'test': test_data,\n",
    "    'valid': valid_data\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [label for label in dataset['train'].features.keys() if label not in ['index', 'name','author','text']]\n",
    "id2label = {idx:label for idx, label in enumerate(labels)}\n",
    "label2id = {label:idx for idx, label in enumerate(labels)}\n",
    "\n",
    "assert len(labels) == 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d1f3e99d505431da8f90854b8ec7f4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/11368 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97e650692c914d879c5213b0804b177f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/842 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a54a47960a64a678c62dd81e8c0be74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def prepare_data(batch):\n",
    "    text = batch['text']\n",
    "    encoding = tokenizer(text,padding='max_length', truncation=True, max_length=512)\n",
    "    labels_batch = {key: batch[key] for key in batch.keys() if key in labels}\n",
    "    labels_matrix = np.zeros((len(text),len(labels)))\n",
    "    for idx, label in enumerate(labels):\n",
    "        labels_matrix[:,idx] = labels_batch[label]\n",
    "\n",
    "    encoding['labels'] = labels_matrix.tolist()\n",
    "    \n",
    "    return encoding\n",
    "encoded_dataset = dataset.map(prepare_data,batched=True, remove_columns=dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL BUILD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda'\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"content/models/distilbert-book-classifier/checkpoint-4977\"\n",
    ")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    f\"TxMM/models/distilbert-book-classifier\",\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_label_metrics(predictions, labels):\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probabilities = sigmoid(torch.Tensor(predictions))\n",
    "    y_pred = np.zeros(probabilities.shape)\n",
    "    y_true = labels\n",
    "    y_pred[np.where(probabilities>=0.5)] = 1\n",
    "    f1_micro = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc_micro = roc_auc_score(y_true=y_true, y_score=y_pred, average='micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    f1_weight = f1_score(y_true=y_true, y_pred=y_pred, average='weighted')\n",
    "    roc_auc_weight = roc_auc_score(y_true=y_true, y_score=y_pred, average='weighted')\n",
    "\n",
    "    metrics = {\n",
    "        \"f1\":f1_micro,\n",
    "        \"roc_auc\":roc_auc_micro,\n",
    "        \"accuracy\":accuracy,\n",
    "        \"f1 weights\":f1_weight,\n",
    "        \"roc weights\":roc_auc_weight,\n",
    "              }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def compute_metrics(p:EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions,tuple) else p.predictions\n",
    "    result= multi_label_metrics(\n",
    "        predictions=preds,\n",
    "        labels=p.label_ids\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=tensor(0.1111, device='cuda:0',\n",
       "       grad_fn=<BinaryCrossEntropyWithLogitsBackward0>), logits=tensor([[-3.1344, -4.3958, -6.3208, -3.7092, -4.2933, -4.0962, -4.0419, -3.0307,\n",
       "         -5.1862, -0.0478, -5.1422, -3.2470, -5.9466, -2.2737, -1.2610, -4.6415,\n",
       "         -4.1033, -1.2608, -3.3578, -0.1504]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(input_ids=encoded_dataset['train']['input_ids'][0].unsqueeze(0).to(device), labels=encoded_dataset['train'][0]['labels'].unsqueeze(0).to(device))\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=encoded_dataset['train'],\n",
    "    eval_dataset=encoded_dataset['valid'],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = trainer.predict(encoded_dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[-4.601303  , -5.6358523 , -5.12385   , ..., -3.0735242 ,\n",
       "        -5.060347  , -5.9021487 ],\n",
       "       [-5.126658  , -2.610917  , -2.8949027 , ..., -0.11496418,\n",
       "        -4.8210545 , -3.7965467 ],\n",
       "       [-4.3129416 , -4.693411  , -6.271017  , ..., -1.3670057 ,\n",
       "        -4.4582853 , -4.690391  ],\n",
       "       ...,\n",
       "       [-4.116854  , -1.6124333 , -5.3117094 , ..., -2.2691333 ,\n",
       "        -3.4601767 , -3.9122982 ],\n",
       "       [-4.7879944 , -1.201875  , -2.8339856 , ..., -3.3609045 ,\n",
       "        -3.9086614 , -3.8135092 ],\n",
       "       [-4.4846244 ,  2.0346642 , -5.1685324 , ..., -4.1326036 ,\n",
       "        -0.5097189 , -3.387729  ]], dtype=float32), label_ids=array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]], dtype=float32), metrics={'test_loss': 0.16784217953681946, 'test_f1': 0.4918032786885246, 'test_roc_auc': 0.7324750393289755, 'test_accuracy': 0.3824228028503563, 'test_f1 weights': 0.48792126347505416, 'test_roc weights': 0.7287616961832093, 'test_runtime': 26.8721, 'test_samples_per_second': 31.334, 'test_steps_per_second': 1.972})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "InterAceAge = \"\".join([\n",
    "    \"The future is forever a projection of the present. \",\n",
    "    \"Something whose connection with human experience we cannot grasp is bound to be frightening. \",\n",
    "    \"Perhaps there is no such thing as a cruel future. The future, properly speaking, is already cruel by virtue of being the future. The responsibility for this cruelty lies not on the side of the future, but on that of a present unable to accept the abyss that separates the two.\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index': 14179,\n",
       " 'name': 'Tales of a Fourth Grade Nothing',\n",
       " 'author': 'Judy Blume',\n",
       " 'text': 'on the rug. What’s next on your reading list? Discove. I thought how great it would be if we could trade in Fudge for a nice cocker spaniel. birthday party. And even more, I’m going to see to it that he’s happy. One night my father came home from the office all excited. He told us Mr. and Mrs. Yarby were coming to New York. He’s the president of the Juicy-O company. H. Hallowe’en. Fudge. If I decided not to eat they’d probably never even notice. dope-pushers hang around there. But taking dope is even dumber than smoking, so nobody’s going to hook me! We live o. He’s really fat. Wear or wrap. May tenth. How do, Peter, ” Mr. Yarby said. Mrs. Yarby just gave me a nod. windup train that made a lot of noise. Every time it bumped into something it turned around and went the other way. Fudge liked it a lot. He likes anything that’s noisy. Ralph arrived first. He’s really fat. And he isn’t even four years old. He doesn’t say much either. He grunts and grabs a lot, though. Usually his mouth is stuffed full of something. T. Nobody ever worries about me the way they worry about Fudge. If I decided not to eat they’d probably never even notice. Jennie had a big smile on her face. Next thing I knew there was a puddle o. fourth grade nothing. . Berman’s foot measure. Then he turned it around and I put my right foot in. That’s another reason why my mother thinks Mr. Berman is good at sellin. 1 The Big Winne. thought how great it would be if we coul. Jimmy Fargo’s birthday party. All the other guys got to take home goldfish in little plastic bags. I won him because I guessed there were three hundred and forty-eight jelly beans in Mrs. Fargo’s jar. Really. My biggest problem is my brother, Farley Drexel Hatcher. He’s two-and-a-half years old. Everybody calls him Fudge. . read. Not me. me to put him up on Fudge’s new bed while she took the rest of the children back to the living room',\n",
       " 'Business-Finance-Law': False,\n",
       " 'Religion': False,\n",
       " 'Natural-History': False,\n",
       " 'Humour': False,\n",
       " 'Computing': False,\n",
       " 'Medical': False,\n",
       " 'Health': False,\n",
       " 'Mind-Body-Spirit': False,\n",
       " 'Crime-Thriller': False,\n",
       " 'Technology-Engineering': False,\n",
       " 'Romance': False,\n",
       " 'Entertainment': False,\n",
       " 'Teen-Young-Adult': True,\n",
       " 'Science-Geography': False,\n",
       " 'Personal-Development': False,\n",
       " 'Biography': False,\n",
       " 'Science-Fiction-Fantasy-Horror': False,\n",
       " 'Poetry-Drama': False,\n",
       " 'History-Archaeology': False,\n",
       " 'Society-Social-Sciences': False}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
